# Multi-Threaded_WebCrawler
This is a 'crawler' to 'crawl' (visit, fetch contents of) a list of URLs, starting with an empty queue and a seed website, then recursively shrink/grow it (remove items, possibly add new items), till the queue is empty again.
